{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DriftKing-byte/QA-Chatbot/blob/main/Airline_Ticket_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgkXotWC-QvQ",
        "outputId": "ad455b84-9140-496e-8bfa-df57989b370e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irc7yMVj-UYv",
        "outputId": "776f05c0-4646-460d-8755-057bb175fe76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.563838513999997\n",
            "GPU (s):\n",
            "0.12208935200004589\n",
            "GPU speedup over CPU: 29x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDYXIyTHc9jn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "DZsBVmSvxlvO",
        "outputId": "f3ba50c2-c9d9-4633-a651-5085d24c78d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-880d196b-dd25-4633-8e8f-4345e342111d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-880d196b-dd25-4633-8e8f-4345e342111d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2491188199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WgQxhFJz9k5"
      },
      "outputs": [],
      "source": [
        "airline_data = pd.read_csv(io.BytesIO(uploaded['airline.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jjuqt4l04Rs"
      },
      "outputs": [],
      "source": [
        "airline_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I83g2bBe1s2-"
      },
      "outputs": [],
      "source": [
        "y = airline_data.price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4ithduxeuBY"
      },
      "outputs": [],
      "source": [
        "pp_d1 = airline_data[['airline','source_city','departure_time','stops','arrival_time','destination_city', 'duration', 'days_left']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUwAxg9yfkk3"
      },
      "outputs": [],
      "source": [
        "pp_d1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlPuHJyA5OrA"
      },
      "outputs": [],
      "source": [
        "##these columns can be one hot encoded. The other columns should be either removed or extracted and then one hot encoded. Add all these encoded stuff\n",
        "## to final X.\n",
        "pp_d1_mod = pd.get_dummies(pp_d1, columns = ['airline','source_city','departure_time','stops','arrival_time','destination_city'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez4ACj4UgMyA"
      },
      "outputs": [],
      "source": [
        "#check to see if 1 and 0 filled in all the columns\n",
        "pp_d1_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe1Q-ClwgSu2"
      },
      "outputs": [],
      "source": [
        "#Didn't happen so we have to use the following code to change the type of output data\n",
        "pp_d1_mod2 = pp_d1_mod.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfjSrkRngzuY"
      },
      "outputs": [],
      "source": [
        "pp_d1_mod2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfQgisbA7oJU"
      },
      "outputs": [],
      "source": [
        "## we have remaining 'flight' and 'class'; string extraction and encoding will be done to flight. ordinal encoding will be done to class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD3Wc53mQWcE"
      },
      "outputs": [],
      "source": [
        "#to extract first part of string in flight column\n",
        "pp_d1_mod2['flight'] = airline_data['flight'].str.split('-').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVaudBNnhaJ6"
      },
      "outputs": [],
      "source": [
        "pp_d1_mod2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o53oMnGhqRK"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod = pd.get_dummies(pp_d1_mod2, columns = ['flight'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zldARC2_kImv"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJgravaNkRVV"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod = pp_d3_mod.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6-0rftOkZ61"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHSG3kKElJvs"
      },
      "outputs": [],
      "source": [
        "#class is all that's left now. We have to change to ordinal ranking. We can do this using scikitlearn.\n",
        "airline_data['class'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bpCSlcil5CI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "class_order = [['Economy','Business']]\n",
        "encoder = OrdinalEncoder(categories=class_order)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oHseqMln7Af"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod['class'] = airline_data['class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5mKr1x6oIH7"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fs6WYjVoLiy"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod['class'] = encoder.fit_transform(pp_d3_mod[['class']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4xvgOCcrK5O"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHKP4QycpMJG"
      },
      "outputs": [],
      "source": [
        "# adding columns of one dataframe to another\n",
        "pp_d3_mod[['duration', 'days_left']] = airline_data[['duration', 'days_left']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBlaf55KtIao"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33rg7WIrB_rz"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod['price'] = airline_data['price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFPijBRWC2Tm"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRiPbJtUMAM6"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSX2QmDcD8vp"
      },
      "outputs": [],
      "source": [
        "pp_d3_mod.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXdzOo7FC8DZ"
      },
      "outputs": [],
      "source": [
        "y = pp_d3_mod.price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8eJJ-glDPMi"
      },
      "outputs": [],
      "source": [
        "features = ['airline_AirAsia', 'airline_Air_India', 'airline_GO_FIRST',\n",
        "       'airline_Indigo', 'airline_SpiceJet', 'airline_Vistara',\n",
        "       'source_city_Bangalore', 'source_city_Chennai', 'source_city_Delhi',\n",
        "       'source_city_Hyderabad', 'source_city_Kolkata', 'source_city_Mumbai',\n",
        "       'departure_time_Afternoon', 'departure_time_Early_Morning',\n",
        "       'departure_time_Evening', 'departure_time_Late_Night',\n",
        "       'departure_time_Morning', 'departure_time_Night', 'stops_one',\n",
        "       'stops_two_or_more', 'stops_zero', 'arrival_time_Afternoon',\n",
        "       'arrival_time_Early_Morning', 'arrival_time_Evening',\n",
        "       'arrival_time_Late_Night', 'arrival_time_Morning', 'arrival_time_Night',\n",
        "       'destination_city_Bangalore', 'destination_city_Chennai',\n",
        "       'destination_city_Delhi', 'destination_city_Hyderabad',\n",
        "       'destination_city_Kolkata', 'destination_city_Mumbai', 'flight_6E',\n",
        "       'flight_AI', 'flight_G8', 'flight_I5', 'flight_SG', 'flight_UK',\n",
        "       'class', 'duration', 'days_left']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGXAKgYNEL_B"
      },
      "outputs": [],
      "source": [
        "X = pp_d3_mod[features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ncwtM3CEpn2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkCG0KC9EMU"
      },
      "source": [
        "Below is using random forest regressor. Skip to below correlation heatmap for keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqejT5g9FJMg"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "airline_tree = RandomForestRegressor(random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEaxBidzFeQq"
      },
      "outputs": [],
      "source": [
        "# Takes 2 minutes to run\n",
        "airline_tree.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYRUvRoRFslq"
      },
      "outputs": [],
      "source": [
        "airline_pred = airline_tree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heDkMPcvF3Ty"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "val_mae = mean_absolute_error(y_test, airline_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBa9FG80GMic"
      },
      "outputs": [],
      "source": [
        "print(\"Validation MAE for Random Forest Model: {}\".format(val_mae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D5wFveOdpCw"
      },
      "source": [
        "~2000 dollars difference. Random Forest Regressor works best for tabular. Non-tabular would be keras and tensorflow. XGBoost gives similar results. CNNs work best for non-tabular as said earlier. Try SVR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import cupy as cp\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "x_train_cp = cp.asarray(X_train_scaled)\n",
        "x_test_cp = cp.asarray(X_test_scaled)\n",
        "y_train_cp = cp.asarray(y_train)"
      ],
      "metadata": {
        "id": "PDpPzZXl6_yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "from cuml.svm import SVR as cuSVR\n",
        "\n",
        "model = cuSVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "model.fit(x_train_cp, y_train_cp)"
      ],
      "metadata": {
        "id": "QFbL-vKt7W1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cp = model.predict(x_test_cp)\n",
        "\n",
        "y_pred_np = cp.asnumpy(y_pred_cp)"
      ],
      "metadata": {
        "id": "5DAkzR2W9OOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "val_mae = mean_absolute_error(y_test, y_pred_np)\n",
        "\n",
        "print(val_mae)"
      ],
      "metadata": {
        "id": "WlXgAmAa-AGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is GPU based algorithm for svr. Accuracy is still low."
      ],
      "metadata": {
        "id": "dDCf6zLbBKNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is XGBoost implementation."
      ],
      "metadata": {
        "id": "tc4w7RT_BO3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "model = xgb.XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "rVdnQXfL9ias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "AClWi1XkBnOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "c-hIrqTgBrfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "3PSt7BfzBx2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHXmwLoRYu7V"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svr = SVR()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkq2AaLU5fsU"
      },
      "outputs": [],
      "source": [
        "svr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8ktafAEtxlS"
      },
      "outputs": [],
      "source": [
        "y_pred = svr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG4OXgU9uJ38"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "print(mean_absolute_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYWrlbkbhCaQ"
      },
      "outputs": [],
      "source": [
        "# Accuracy is used for classification but not regression models. Regression models have MAE instead among others but not accuracy, precision, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ar0Os22Lo8T"
      },
      "outputs": [],
      "source": [
        "bill## Now, it is time to get the parameters and hypertune!\n",
        "airline_tree.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wQI6L2YfWZf"
      },
      "source": [
        "Apparently SGD regressor works better than RandomTreeRegressor with data over 100K samples. There is more preprocessing to do probably and differs. So, find out what it is. You can hypertune that as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA7jvifdkpHc"
      },
      "outputs": [],
      "source": [
        "## A heatmap shows non-linearity. Tree regressor seems much better than sgd regressor so it wasn't explored. The map can't be done before\n",
        "#bcz everythong should be float/int\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "    # Assuming 'X' is your feature DataFrame and 'y' is your target Series\n",
        "pp_d3_mod = pd.concat([X, y], axis=1)\n",
        "correlation_matrix = pp_d3_mod.corr()\n",
        "plt.figure(figsize=(55, 55))\n",
        "sns.heatmap(correlation_matrix, annot_kws={\"fontsize\": 14, \"fontweight\": 'bold'}, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "print(correlation_matrix['price']) #print the correlation of each feature with price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMjCuRSCpWtE"
      },
      "source": [
        "### Conclusion: 1) High MAE (i.e. 1070) 2) Low correlation of features to label 3) Runtime ~2 min."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvysvaIcpy_M"
      },
      "source": [
        "### For future: 1) Find highly correlated data if possible and/or time allows (for a good project) 2) Allows for low MAE 3) Runtime might not change but is not issue now. Focus on hypertuning instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZi-mzyd1E1"
      },
      "source": [
        "Below is keras and tensorflow application which is not accurate because their applications work best for non-tabular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGYugW6D9NS0"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os3OnGHp9rr7"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-1DCpdd9xvQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzERdlLf92hs"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ6HjyzM-GOF"
      },
      "outputs": [],
      "source": [
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19f2BbAY-Qam"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxBUBJuq-MYK"
      },
      "outputs": [],
      "source": [
        "X_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCzRXBI_-hCc"
      },
      "outputs": [],
      "source": [
        "X_train.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnTkQ8l9-t56"
      },
      "outputs": [],
      "source": [
        "X_test.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv0dnDYv-vS8"
      },
      "outputs": [],
      "source": [
        "X_train.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LldIR33D-xfQ"
      },
      "outputs": [],
      "source": [
        "X_test.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv550hu3-8-A"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQW5KGN4_OUP"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(42, activation='relu'))\n",
        "model.add(Dense(21, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer = 'adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBCJNJ6yNFNg"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, batch_size = 128, epochs = 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtmME2SfwmPgJSdTedwafK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}